{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1084fa32",
   "metadata": {},
   "source": [
    "## Important Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b0b4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType,DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import current_date, date_sub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, sum as spark_sum, isnan, when\n",
    "from pyspark.sql.functions import to_timestamp,to_date,count\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import desc, sum as sum_agg\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, datediff, sum as spark_sum, count, max as spark_max\n",
    "from datetime import date\n",
    "import scipy.stats as stat\n",
    "import pylab  #,clust_plot\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import sys\n",
    "import yellowbrick\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77022998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing libraries as i was working on a new python virtual environment \n",
    "\n",
    "# #pip install prettytable\n",
    "# !pip install yellowbrick\n",
    "# #!pip install pandas\n",
    "# #!pip install findspark\n",
    "# #!pip install seaborn\n",
    "# !pip install scipy\n",
    "#!pip install --upgrade pandas\n",
    "#!pip install --upgrade --force-reinstall pandas\n",
    "#!pip install dash dash-bootstrap-components pandas plotly pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f99bc-86ad-4204-a0a9-c4c0467a0648",
   "metadata": {},
   "source": [
    "## Dash Related Configuration for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00304a-7a91-4cf6-b54e-b8846b0bc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dash related imports\n",
    "\n",
    "import dash\n",
    "# import dash_core_components as dcc\n",
    "# import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "from dash import Dash, dcc, html\n",
    "\n",
    "#import pandas as pd\n",
    "import plotly.express as px\n",
    "#from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11af8c2-efe6-4ac6-a931-d6390ffa32c9",
   "metadata": {},
   "source": [
    "## Visualization using Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc83cd-1618-4b98-8225-a174ace5d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a Dash app with interactive slides to visualize customer segmentation using Spark.\n",
    "# The app's layout consists of a header and a slider to switch between plots.\n",
    "# The data visualizations are displayed in the callback's updated picture, which is dependent on the slider's position.\n",
    "# I have made modular functions which contains the images generated from the plots and integrated with the das app by using interactive slides.  \n",
    "\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1('Visualization of Customer Segmentation using Spark'),\n",
    "    html.Div(id='slides-container', style={'height': '90vh', 'background-color': 'white'}),\n",
    "    dcc.Slider(\n",
    "        id='slide-slider',\n",
    "        min=0,\n",
    "        max=6,\n",
    "        step=1,\n",
    "        value=0,\n",
    "        marks={i: f'Slide {i+1}' for i in range(7)}  # More Pythonic way to define marks\n",
    "    ),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('slides-container', 'children'),\n",
    "    [Input('slide-slider', 'value')]\n",
    ")\n",
    "def update_slide(value):\n",
    "    if value == 0:\n",
    "        image_path = 'missing_values_plot.png'\n",
    "        return html.Img(src='data:image/png;base64,{}'.format(base64.b64encode(open(image_path, 'rb').read()).decode()), style={'width': '100%', 'height': '100%', 'object-fit': 'contain'})\n",
    "    elif value in range(1, 7):\n",
    "        feature = ['Recency', 'Recency_Boxcox', 'Frequency', 'Frequency_log', 'Monetary', 'Monetary_log'][value-1]\n",
    "        image_path = generate_plot(rfm_df_pd, feature)\n",
    "        return html.Img(src='data:image/png;base64,{}'.format(base64.b64encode(open(image_path, 'rb').read()).decode()), style={'width': '100%', 'height': '100%', 'object-fit': 'contain'})\n",
    "    else:\n",
    "        return html.H2('Slide not found')\n",
    "\n",
    "def generate_plot(df, feature):\n",
    "    binf(df, feature)\n",
    "    plot_path = f'{feature}_plot.png'\n",
    "    return plot_path\n",
    "\n",
    "def binf(df, feature):\n",
    "    binned_data = pd.cut(df[feature], bins=4)\n",
    "    frequency = binned_data.value_counts()\n",
    "    sorted_frequency = frequency.sort_index(ascending=False)\n",
    "    plt.figure(figsize=(10, 6), dpi=100)  \n",
    "    sorted_frequency.plot(kind='bar')\n",
    "    plt.xlabel('Bins')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency of Data Points in Each Bin: {feature}')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels to prevent overlap\n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.savefig(f'{feature}_plot.png', dpi=100, bbox_inches='tight')  \n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e6b05-dbcb-4b90-a09c-9c47a736e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2ea5a16-1c44-4802-9c75-17a64d9c35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.3.0\n",
      "Pandas version: 2.2.2\n",
      "NumPy version: 1.24.3\n",
      "Matplotlib version: 3.9.1\n",
      "Seaborn version 0.13.2\n",
      "Python 3.11.8\n"
     ]
    }
   ],
   "source": [
    "print(\"PySpark version:\", pyspark.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "#print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "\n",
    "# Now you can print the Matplotlib version without any NameError\n",
    "print(\"Matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"Seaborn version\", sns.__version__)\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088816c",
   "metadata": {},
   "source": [
    "### Pyspark home directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb6e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = \"spark_home_directory\"\n",
    "\n",
    "\n",
    "findspark.init(spark_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092b662",
   "metadata": {},
   "source": [
    "### Initialize SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b47870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HadoopSparkIntegration\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "133ca1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HadoopSparkIntegration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x162afe390>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6ebc1-f510-451f-98b4-221f69f379b1",
   "metadata": {},
   "source": [
    "## Testing the integration of Hadoop and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa9a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to a text file\n",
    "text_data = \"Hello, Hadoop and Spark integration!\"\n",
    "with open(\"temp_H.txt\", \"w\") as file:\n",
    "    file.write(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baed2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Execute HDFS put command from Python\n",
    "subprocess.run([\"hdfs\", \"dfs\", \"-put\", \"temp_H.txt\", \"/test/hadoop_spark_test_H.txt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"hdfs\", \"dfs\", \"-ls\", \"/test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43733041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run([\"hdfs\", \"dfs\", \"-cat\", \"/test/hadoop_spark_test_H.txt\"], capture_output=True)\n",
    "print(result.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe4dc1",
   "metadata": {},
   "source": [
    "## Uploading data in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the root of HDFS\n",
    "!hdfs dfs -ls /\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ccca7-2b6d-4b5d-90dd-e2b935b6d064",
   "metadata": {},
   "source": [
    "### Creating a new directory in HDFS called project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f41178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new directory in hdfs for the project\n",
    "\n",
    "!hdfs dfs -mkdir /project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the root of HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the CSV file from local system to the 'project' directory in HDFS\n",
    "!hdfs dfs -put local_csv_path /project/testing_csv.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cca3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the 'project' directory to confirm the file is there\n",
    "!hdfs dfs -ls /project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f281b-27df-4503-9d00-0a14df06dcf8",
   "metadata": {},
   "source": [
    "### The below commented out code contains how I faced issues while I tried using inferschema and eventually designed a custom_schema according to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1abb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('Read CSV from HDFS Project Directory') \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Read the CSV file from the new directory in HDFS with the fully specified URI\n",
    "# # Ensure to include options for handling headers and schema inference\n",
    "# df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "#     .csv(\"hdfs://localhost:9000/project/testing_csv.csv\")\n",
    "\n",
    "# # Show the first few rows to ensure it's loaded correctly\n",
    "# df.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7fbe111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "951f2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I will now try importing with my defined schmea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73032c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This schema has incorrect data schema\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# # Define the schema correctly\n",
    "# custom_schema = StructType([\n",
    "#     StructField(\"TransactionID\", StringType(), True),  # Changed to StringType\n",
    "#     StructField(\"CustomerID\", StringType(), True),\n",
    "#     StructField(\"CustomerDOB\", DateType(), True),\n",
    "#     StructField(\"CustGender\", StringType(), True),\n",
    "#     StructField(\"CustLocation\", StringType(), True),\n",
    "#     StructField(\"CustAccountBalance\", DoubleType(), True),\n",
    "#     StructField(\"TransactionDate\", DateType(), True),\n",
    "#     StructField(\"TransactionTime\", IntegerType(), True),  # Assuming it's in HHMMSS format\n",
    "#     StructField(\"TransactionAmount (INR)\", DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# # Initialize SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('Read CSV from HDFS Project Directory') \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Read the CSV file using the schema defined and correct date format\n",
    "# df = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"dateFormat\", \"d/M/yy\") \\  \n",
    "#     .schema(custom_schema) \\\n",
    "#     .load(\"hdfs://localhost:9000/project/testing_csv.csv\")\n",
    "\n",
    "# # Show the first few rows to ensure it's loaded correctly\n",
    "# df.show()\n",
    "\n",
    "# # Print the schema to verify the types are as expected\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bded0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''In this code chunk I have imported using my custom schema and initially faced issues with the date format and then\n",
    "# used config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") to help parse the two columns CustomerDOB and TransactionDate in the yyyy-MM-dd format'''\n",
    "\n",
    "# #Custom Schema\n",
    "# custom_schema = StructType([\n",
    "#     StructField(\"TransactionID\", StringType(), True),  \n",
    "#     StructField(\"CustomerID\", StringType(), True),\n",
    "#     StructField(\"CustomerDOB\", DateType(), True),\n",
    "#     StructField(\"CustGender\", StringType(), True),\n",
    "#     StructField(\"CustLocation\", StringType(), True),\n",
    "#     StructField(\"CustAccountBalance\", DoubleType(), True),\n",
    "#     StructField(\"TransactionDate\", DateType(), True),\n",
    "#     StructField(\"TransactionTime\", IntegerType(), True), \n",
    "#     StructField(\"TransactionAmount (INR)\", DoubleType(), True),\n",
    "# ])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('Read CSV from HDFS Project Directory') \\\n",
    "#     .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# df = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"dateFormat\", \"d/M/yy\") \\\n",
    "#     .schema(custom_schema) \\\n",
    "#     .load(\"hdfs://localhost:9000/project/testing_csv.csv\")\n",
    "\n",
    "# df.show(20)\n",
    "\n",
    "# df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the root of HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511853fb-9cb9-4aca-ba1e-9215aff1ba52",
   "metadata": {},
   "source": [
    "### Checking if we could load 100,000 records to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the 100,000 records into hdfs and testing it for our project\n",
    "\n",
    "# Upload the CSV file from local system to the 'project' directory in HDFS\n",
    "!hdfs dfs -put file_path_to/testing_hund.csv /project/testing_hund.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8285ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the 'project' directory to confirm the file is there\n",
    "!hdfs dfs -ls /project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d51789",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using thes same configuartions and schema for importing the 100,000 records\n",
    "\n",
    "'''In this code chunk I have imported using my custom schema and initially faced issues with the date format and then\n",
    "used config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") to help parse the two columns CustomerDOB and TransactionDate in the yyyy-MM-dd format'''\n",
    "\n",
    "#Custom Schema for hundred thousand records \n",
    "custom_schema_hund = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),  \n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerDOB\", DateType(), True),\n",
    "    StructField(\"CustGender\", StringType(), True),\n",
    "    StructField(\"CustLocation\", StringType(), True),\n",
    "    StructField(\"CustAccountBalance\", DoubleType(), True),\n",
    "    StructField(\"TransactionDate\", DateType(), True),\n",
    "    StructField(\"TransactionTime\", IntegerType(), True), \n",
    "    StructField(\"TransactionAmount (INR)\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName('Read CSV from HDFS Project Directory') \\\n",
    "#     .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "df_hund = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"d/M/yy\") \\\n",
    "    .schema(custom_schema_hund) \\\n",
    "    .load(\"hdfs://localhost:9000/project/testing_hund.csv\")\n",
    "\n",
    "#df_hund.show(25)\n",
    "\n",
    "df_hund.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20520c3d",
   "metadata": {},
   "source": [
    "## Uploading full data in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the root of HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uploading our full data which contains 1m+ records to hdfs\n",
    "\n",
    "# Upload the CSV file from local system to the 'project' directory in HDFS\n",
    "!hdfs dfs -put file_path_to/full_data.csv /project/full_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaedf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the 'project' directory to confirm the file is there\n",
    "!hdfs dfs -ls /project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d108261c-ea08-4f27-8ddc-40233b2048cb",
   "metadata": {},
   "source": [
    "### Import the project data (bank_transactions.csv) from HDFS and read it using our custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using thes same configuartions and schema for all the records\n",
    "\n",
    "'''In this code chunk I have imported using my custom schema and initially faced issues with the date format and then\n",
    "used config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") to help parse the two columns CustomerDOB and TransactionDate in the yyyy-MM-dd format'''\n",
    "\n",
    "#Custom Schema for hundred thousand records \n",
    "custom_schema_full = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),  \n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerDOB\", DateType(), True),\n",
    "    StructField(\"CustGender\", StringType(), True),\n",
    "    StructField(\"CustLocation\", StringType(), True),\n",
    "    StructField(\"CustAccountBalance\", DoubleType(), True),\n",
    "    StructField(\"TransactionDate\", DateType(), True),\n",
    "    StructField(\"TransactionTime\", IntegerType(), True), \n",
    "    StructField(\"TransactionAmount (INR)\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "df_full = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"d/M/yy\") \\\n",
    "    .schema(custom_schema_full) \\\n",
    "    .load(\"hdfs://localhost:9000/project/full_data.csv\")\n",
    "\n",
    "df_full.show(25,False)\n",
    "\n",
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195adee1-036d-41eb-8eb0-49a747939cdc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62a8ca-76cd-4015-8d0a-14703087546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a glance at the datframe before renaming columns\n",
    "\n",
    "df_full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95c66d-3106-413f-b037-5991172d3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The Following columns which are renamed are : \n",
    "\n",
    "CustomerDOB : DOB\n",
    "CustGender : Gender\n",
    "CustLocation: Location\n",
    "CustAccountBalance: AccountBalanace\n",
    "TransactionDate : Date\n",
    "TransactionTime : Time\n",
    "TransactionAmount (INR) : Amount'''\n",
    "\n",
    "# Rename the columns\n",
    "data_eda = df_full.withColumnRenamed(\"CustomerDOB\", \"DOB\") \\\n",
    "       .withColumnRenamed(\"CustGender\", \"Gender\") \\\n",
    "       .withColumnRenamed(\"CustLocation\", \"Location\") \\\n",
    "       .withColumnRenamed(\"CustAccountBalance\", \"AccountBalance\") \\\n",
    "       .withColumnRenamed(\"TransactionDate\", \"Date\") \\\n",
    "       .withColumnRenamed(\"TransactionTime\", \"Time\") \\\n",
    "       .withColumnRenamed(\"TransactionAmount (INR)\", \"Amount\")\n",
    "\n",
    "\n",
    "# Show the result to verify the changes\n",
    "data_eda.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98869653-74a5-4ccc-9307-fb24fc184805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame\n",
    "print(\"Number of rows:\", data_eda.count())\n",
    "print(\"Number of columns:\", len(data_eda.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7a848-0e83-43bd-9741-efbda667f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check(data_eda):\n",
    "#     data = []\n",
    "#     columns = data_eda.columns\n",
    "#     for c in columns:\n",
    "#         dtypes = str(data_eda.schema[c].dataType)\n",
    "#         nunique = data_eda.select(c).distinct().count()\n",
    "#         sum_null = data_eda.filter(col(c).isNull()).count()\n",
    "#         data.append((c, dtypes, nunique, sum_null))\n",
    "\n",
    "#     schema = StructType([\n",
    "#         StructField(\"column\", StringType(), True),\n",
    "#         StructField(\"dtypes\", StringType(), True),\n",
    "#         StructField(\"nunique\", IntegerType(), True),\n",
    "#         StructField(\"sum_null\", IntegerType(), True)\n",
    "#     ])\n",
    "\n",
    "#     df_check = spark.createDataFrame(data, schema)\n",
    "\n",
    "#     # Convert the Spark DataFrame to Pandas for plotting\n",
    "#     df_check_pandas = df_check.toPandas()\n",
    "\n",
    "#     # Filter columns with missing values\n",
    "#     df_check_pandas_filtered = df_check_pandas[df_check_pandas[\"sum_null\"] > 0]\n",
    "\n",
    "#     # Set the plotting style\n",
    "#     sns.set(style=\"whitegrid\")\n",
    "\n",
    "#     # Plot missing values\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(x=\"sum_null\", y=\"column\", data=df_check_pandas_filtered)\n",
    "#     plt.xlabel(\"Number of Missing Values\", fontsize=14)\n",
    "#     plt.ylabel(\"Columns\", fontsize=14)\n",
    "#     plt.title(\"Missing Values per Column\", fontsize=16)\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "#     # Display the missing values table\n",
    "#     print(\"Missing Values Table:\")\n",
    "#     df_check.show()\n",
    "\n",
    "# # Call the function to check missing values\n",
    "# check(data_eda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd799f5-77ed-4e10-8d4b-d70f36f8872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check(data_eda):\n",
    "    data = []\n",
    "    columns = data_eda.columns\n",
    "    for c in columns:\n",
    "        dtypes = str(data_eda.schema[c].dataType)\n",
    "        nunique = data_eda.select(c).distinct().count()\n",
    "        sum_null = data_eda.filter(col(c).isNull()).count()\n",
    "        data.append((c, dtypes, nunique, sum_null))\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"column\", StringType(), True),\n",
    "        StructField(\"dtypes\", StringType(), True),\n",
    "        StructField(\"nunique\", IntegerType(), True),\n",
    "        StructField(\"sum_null\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    df_check = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Convert the Spark DataFrame to Pandas for plotting\n",
    "    df_check_pandas = df_check.toPandas()\n",
    "\n",
    "    # Filter columns with missing values\n",
    "    df_check_pandas_filtered = df_check_pandas[df_check_pandas[\"sum_null\"] > 0]\n",
    "\n",
    "    # Set the plotting style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Plot missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"sum_null\", y=\"column\", data=df_check_pandas_filtered)\n",
    "    plt.xlabel(\"Number of Missing Values\", fontsize=14)\n",
    "    plt.ylabel(\"Columns\", fontsize=14)\n",
    "    plt.title(\"Missing Values per Column\", fontsize=16)\n",
    "    \n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Define the path to save the image file\n",
    "    plot_path = os.path.join(current_dir, \"missing_values_plot.png\")\n",
    "\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    # Close the plot to avoid displaying it in the notebook\n",
    "    plt.close()\n",
    "\n",
    "    # Return the path to the saved image file\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620cf94",
   "metadata": {},
   "source": [
    "## Data Preprocessing using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d139f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For visualization we have created a utility function\n",
    "\n",
    "import sys\n",
    "# This is the directory that directly contains the 'Utility_Folder' which inside it conatins the 'utility.py'\n",
    "sys.path.append('path_to_Utility_Folder')\n",
    "\n",
    "from Utility_Folder.utility import kelbow, clust_plot, plot_data, outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the schema for the df schema.\n",
    "\n",
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb29f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The Following columns which are renamed are : \n",
    "\n",
    "CustomerDOB : DOB\n",
    "CustGender : Gender\n",
    "CustLocation: Location\n",
    "CustAccountBalance: AccountBalanace\n",
    "TransactionDate : Date\n",
    "TransactionTime : Time\n",
    "TransactionAmount (INR) : Amount'''\n",
    "\n",
    "# Rename the columns\n",
    "data = df_full.withColumnRenamed(\"CustomerDOB\", \"DOB\") \\\n",
    "       .withColumnRenamed(\"CustGender\", \"Gender\") \\\n",
    "       .withColumnRenamed(\"CustLocation\", \"Location\") \\\n",
    "       .withColumnRenamed(\"CustAccountBalance\", \"AccountBalance\") \\\n",
    "       .withColumnRenamed(\"TransactionDate\", \"Date\") \\\n",
    "       .withColumnRenamed(\"TransactionTime\", \"Time\") \\\n",
    "       .withColumnRenamed(\"TransactionAmount (INR)\", \"Amount\")\n",
    "\n",
    "\n",
    "# Show the result to verify the changes\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "\n",
    "# Initialize an empty condition for filtering out nulls and NaNs\n",
    "condition = col(data.columns[0]).isNull()\n",
    "for column in data.columns[1:]:  # Start from the second column\n",
    "    # Check data type of the column and apply appropriate condition\n",
    "    if str(data.schema[column].dataType) in ('IntegerType', 'DoubleType', 'FloatType'):\n",
    "        condition |= col(column).isNull() | isnan(col(column))\n",
    "    else:\n",
    "        condition |= col(column).isNull()\n",
    "\n",
    "# Apply the condition to filter the DataFrame\n",
    "filtered_df = data.filter(~condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bcea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filteredn_df contains a condition to fileter out based on a condition.\n",
    "\n",
    "filteredn_df = filtered_df.where((filtered_df['Time'] > 0) & (filtered_df['Amount'] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34545ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the top locations\n",
    "\n",
    "location_counts = filteredn_df.groupBy('Location').agg(count('TransactionID').alias('transaction_count'))\n",
    "total_transactions = filteredn_df.count()\n",
    "location_percentages = location_counts.withColumn('percentage', (col('transaction_count') / total_transactions) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c79f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of all distinct locations\n",
    "\n",
    "ordered_location_percentages = location_percentages.orderBy(desc('percentage'))\n",
    "distinct_locations = ordered_location_percentages.select('Location').distinct()\n",
    "distinct_locations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63915abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the  the top_forty_locations by using the limit function.\n",
    "\n",
    "top_forty_location_percentages = ordered_location_percentages.limit(40)\n",
    "top_forty_location_percentages.show()\n",
    "\n",
    "sums_top_twenty = top_forty_location_percentages.agg(\n",
    "    sum_agg(col('transaction_count')).alias('sum_transaction_count'),\n",
    "    sum_agg(col('percentage')).alias('sum_percentage')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858be79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_locations_list = [row['Location'] for row in top_forty_location_percentages.select('Location').distinct().collect()]\n",
    "df_f = filtered_df.where(filtered_df['Location'].isin(cust_locations_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e106447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7391b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate recency,frequency and monetary\n",
    "latest_date = date(2016, 10, 22)\n",
    "# Define a UDF to calculate the recency in days\n",
    "def calculate_recency(transaction_date):\n",
    "    return (latest_date - transaction_date).days\n",
    "\n",
    "# Register the UDF\n",
    "calculate_recency_udf = udf(calculate_recency, IntegerType())\n",
    "\n",
    "# Group by CustomerID and aggregate\n",
    "rfm_df = df_f.groupBy(\"CustomerID\").agg(\n",
    "    calculate_recency_udf(spark_max(\"Date\")).alias(\"Recency\"),\n",
    "    count(\"TransactionID\").alias(\"Frequency\"),\n",
    "    spark_sum(\"Amount\").alias(\"Monetary\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df_pd = rfm_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b936a-06ab-4775-97aa-2763b5c9544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Recency, Monetary and Frequency for visualization.\n",
    "\n",
    "plot_data(rfm_df_pd,'Recency')\n",
    "plot_data(rfm_df_pd,'Frequency')\n",
    "plot_data(rfm_df_pd,'Monetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab6dbf-c913-4f91-804d-c28803c01327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pefroming normalization of data using log transformation.\n",
    "\n",
    "rfm_df_pd['Recency_Boxcox'],parameters=stat.boxcox(rfm_df_pd['Recency']+1)\n",
    "rfm_df_pd['Monetary_log'] = np.log1p(rfm_df_pd['Monetary'])\n",
    "rfm_df_pd['Frequency_log'] = np.log1p(rfm_df_pd['Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4e467-d62e-4d9d-9f6f-7745b91bdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(rfm_df_pd,'Recency_Boxcox')\n",
    "plot_data(rfm_df_pd,'Frequency_log')\n",
    "plot_data(rfm_df_pd,'Monetary_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f5fd2-7665-41be-975f-13fb8942fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df_pd.describe()#644016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b73a6-c733-450b-abf6-5562ee5140f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "\n",
    "rfm_df_pd = outlier(rfm_df_pd,'Recency')\n",
    "rfm_df_pd = outlier(rfm_df_pd,'Frequency')\n",
    "rfm_df_pd = outlier(rfm_df_pd,'Monetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd881a-cff4-43a4-a377-a1e4e6f14526",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_df_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb230b-836f-4bd1-871f-14615f35a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code snippet for the function below where we are plotting of the frequency of data points in each bin\n",
    "\n",
    "def binf(df,feature):\n",
    "    binned_data = pd.cut(df[feature],bins=4)\n",
    "\n",
    "    frequency = binned_data.value_counts()\n",
    "    sorted_frequency = frequency.sort_index(ascending=False)\n",
    "    sorted_frequency.plot(kind='bar')\n",
    "\n",
    "    plt.xlabel('Bins')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency of Data Points in Each Bin:{feature}')\n",
    "    plt.show()\n",
    "binf(rfm_df_pd,'Recency')\n",
    "binf(rfm_df_pd,'Recency_Boxcox')\n",
    "binf(rfm_df_pd,'Frequency')\n",
    "binf(rfm_df_pd,'Frequency_log')\n",
    "binf(rfm_df_pd,'Monetary')\n",
    "binf(rfm_df_pd,'Monetary_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8350358-c927-4961-9980-e50230526ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rfm_df_pd)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Recency_Boxcox', 'Frequency_log', 'Monetary_log'], outputCol='features')\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e402c-4a00-458c-a788-4aaa1ac3aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = df_scaled.select(\"scaledFeatures\").rdd.map(lambda row: row['scaledFeatures'].toArray())\n",
    "local_vectors = vectors.collect()\n",
    "numpy_array = np.array(local_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64cccd-5919-4c50-834f-b60530538a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finidng the ideal number of clusters.\n",
    "\n",
    "kelbow(numpy_array)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5491f8b-ec49-4066-8cf4-c94308822554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the KMeans Clustering with optimum clusters(which was discovered in the above step). \n",
    "\n",
    "k=4\n",
    "kmeans = KMeans(featuresCol='scaledFeatures', k=k, seed=1, initMode='k-means||', maxIter=1000)\n",
    "model = kmeans.fit(df_scaled)\n",
    "predictions = model.transform(df_scaled)\n",
    "\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='scaledFeatures', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette with squared euclidean distance = {silhouette:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461fae2-1692-4e05-8359-d4a595f8174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc888b-5e00-4f12-afc5-74e3d38edb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The daframe which contains the predictions and the clusters is being showcased below.\n",
    "\n",
    "df_predict = predictions.withColumnRenamed('prediction', 'Cluster')\n",
    "print(df_predict.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181bd43-673e-49b9-b462-cbcf9f47e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the features and converting it toPandas() for forthcoming visualizations.\n",
    "\n",
    "dfp_p = df_predict.toPandas()\n",
    "f1= numpy_array[:,0]\n",
    "f2= numpy_array[:,1]\n",
    "f3=numpy_array[:,2]\n",
    "f4 = dfp_p['Recency_Boxcox']\n",
    "f5 = dfp_p['Frequency_log']\n",
    "f6 = dfp_p['Monetary_log']\n",
    "label_c = dfp_p['Cluster']\n",
    "df = pd.DataFrame({\n",
    "    'Recency': f1,\n",
    "    'Frequency': f2,\n",
    "    'Monetary': f3,\n",
    "    'Recency_Boxcox': f4,\n",
    "    'Frequency_log': f5,\n",
    "    'Monetary_log': f6,\n",
    "    'Clusters': label_c,\n",
    "})\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b3604-dcc9-462f-9acd-e077e107819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b607c7-a459-4289-849f-fc97110d2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A figure illustrating the customer segmentation on Recency and Monetary of all the clusters.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Customer Segmentation based on Recency and Monetary')\n",
    "clusters = df['Clusters'].unique()\n",
    "for cluster in clusters:\n",
    "    subset = df[df['Clusters'] == cluster]\n",
    "    plt.scatter(subset['Recency'], subset['Monetary'], label=cluster, s=50, cmap='Set1')\n",
    "plt.xlabel('Recency')\n",
    "plt.ylabel('Monetary')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bb837-7462-4d37-86a5-f5bd8e64d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary = df_predict.groupBy('Cluster').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': 'mean', 'Cluster': 'count'})\n",
    "cluster_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c05a3-9fa3-458f-84e2-531d7bb875c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the csv's and naming them in this format : fname=0.csv,fname=1.csv,etc.\n",
    "\n",
    "clusters = dfp_p['Cluster'].unique()\n",
    "cols=['CustomerID','Recency','Monetary','Cluster']\n",
    "dfp_p = dfp_p[cols]\n",
    "for cluster in clusters:\n",
    "    subset = dfp_p[dfp_p['Cluster'] == cluster]\n",
    "    filename = f\"fname={cluster}.csv\"\n",
    "    subset.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0189d3-f8fe-497a-8cf4-46845b560052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the df_predict with all the clusters and calucating the mean of Rececy and Monetary and counting the number of entries. Applied the binf function to group all of them into bins for further analysis.\n",
    "\n",
    "cluster_summary = df_predict.groupBy('Cluster').agg({'Recency_Boxcox': 'mean','Monetary_log': 'mean', 'Cluster': 'count'})\n",
    "cluster_summary.show()\n",
    "\n",
    "binf(rfm_df_pd,'Recency_Boxcox')\n",
    "binf(rfm_df_pd,'Monetary_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ee002-b355-4182-aa1f-6cbbd5e33342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfp_p.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d0ab1-452d-422c-836e-8585951b84d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18c529-9f16-4937-8dd4-35e2e26a2b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d135d5e-3194-481f-b672-2cdee347bab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3eb164-eacf-4afe-be13-6c40b076173f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
